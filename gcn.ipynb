{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import load_data, accuracy\n",
    "import numpy as np\n",
    "import  torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "A, X, y, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 2708])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1 / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, input_, adj):\n",
    "        support = torch.mm(input_, self.weight) #mm: 行列積\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"(\" \\\n",
    "                         + str(self.in_features) + \" -> \" \\\n",
    "                        + str(self.out_features) + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dr_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dr_rate = dr_rate\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dr_rate, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fastmode = False\n",
    "seed = 42\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "weight_decay= 5e-4\n",
    "hidden = 16\n",
    "dr_rate = 0.5\n",
    "n_class = len(np.unique(y.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GCN(nfeat=X.shape[1], nhid=hidden, nclass=n_class, dr_rate=dr_rate)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "X = X.to(device)\n",
    "A = A.to(device)\n",
    "y = y.to(device)\n",
    "idx_train = idx_train.to(device)\n",
    "idx_val = idx_val.to(device)\n",
    "idx_test = idx_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    \n",
    "    model.train() #train modeにしている\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X, A)\n",
    "    loss_train = F.nll_loss(output[idx_train], y[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], y[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(X, A)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], y[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], y[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(X, A)\n",
    "    loss_test = F.nll_loss(output[idx_test], y[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], y[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9121 acc_train: 0.3000 loss_val: 1.8914 acc_val: 0.3500 time: 0.0286s\n",
      "Epoch: 0002 loss_train: 1.9024 acc_train: 0.2929 loss_val: 1.8834 acc_val: 0.3500 time: 0.0189s\n",
      "Epoch: 0003 loss_train: 1.8964 acc_train: 0.2929 loss_val: 1.8766 acc_val: 0.3500 time: 0.0187s\n",
      "Epoch: 0004 loss_train: 1.8854 acc_train: 0.2929 loss_val: 1.8698 acc_val: 0.3500 time: 0.0190s\n",
      "Epoch: 0005 loss_train: 1.8839 acc_train: 0.2929 loss_val: 1.8635 acc_val: 0.3500 time: 0.0187s\n",
      "Epoch: 0006 loss_train: 1.8728 acc_train: 0.2929 loss_val: 1.8573 acc_val: 0.3500 time: 0.0186s\n",
      "Epoch: 0007 loss_train: 1.8618 acc_train: 0.2929 loss_val: 1.8511 acc_val: 0.3500 time: 0.0187s\n",
      "Epoch: 0008 loss_train: 1.8628 acc_train: 0.2929 loss_val: 1.8448 acc_val: 0.3500 time: 0.0184s\n",
      "Epoch: 0009 loss_train: 1.8561 acc_train: 0.2929 loss_val: 1.8386 acc_val: 0.3500 time: 0.0189s\n",
      "Epoch: 0010 loss_train: 1.8403 acc_train: 0.2929 loss_val: 1.8323 acc_val: 0.3500 time: 0.0188s\n",
      "Epoch: 0011 loss_train: 1.8342 acc_train: 0.3000 loss_val: 1.8258 acc_val: 0.3500 time: 0.0192s\n",
      "Epoch: 0012 loss_train: 1.8288 acc_train: 0.2929 loss_val: 1.8192 acc_val: 0.3500 time: 0.0259s\n",
      "Epoch: 0013 loss_train: 1.8177 acc_train: 0.2929 loss_val: 1.8125 acc_val: 0.3500 time: 0.0194s\n",
      "Epoch: 0014 loss_train: 1.8099 acc_train: 0.2929 loss_val: 1.8058 acc_val: 0.3500 time: 0.0183s\n",
      "Epoch: 0015 loss_train: 1.8091 acc_train: 0.2929 loss_val: 1.7992 acc_val: 0.3500 time: 0.0190s\n",
      "Epoch: 0016 loss_train: 1.8145 acc_train: 0.2929 loss_val: 1.7927 acc_val: 0.3500 time: 0.0189s\n",
      "Epoch: 0017 loss_train: 1.7972 acc_train: 0.2929 loss_val: 1.7861 acc_val: 0.3500 time: 0.0189s\n",
      "Epoch: 0018 loss_train: 1.7867 acc_train: 0.2929 loss_val: 1.7794 acc_val: 0.3500 time: 0.0185s\n",
      "Epoch: 0019 loss_train: 1.7761 acc_train: 0.2929 loss_val: 1.7727 acc_val: 0.3500 time: 0.0180s\n",
      "Epoch: 0020 loss_train: 1.7534 acc_train: 0.2929 loss_val: 1.7661 acc_val: 0.3500 time: 0.0181s\n",
      "Epoch: 0021 loss_train: 1.7651 acc_train: 0.2929 loss_val: 1.7595 acc_val: 0.3500 time: 0.0202s\n",
      "Epoch: 0022 loss_train: 1.7606 acc_train: 0.2929 loss_val: 1.7527 acc_val: 0.3500 time: 0.0246s\n",
      "Epoch: 0023 loss_train: 1.7563 acc_train: 0.2929 loss_val: 1.7460 acc_val: 0.3500 time: 0.0201s\n",
      "Epoch: 0024 loss_train: 1.7409 acc_train: 0.2929 loss_val: 1.7394 acc_val: 0.3500 time: 0.0185s\n",
      "Epoch: 0025 loss_train: 1.7458 acc_train: 0.3071 loss_val: 1.7330 acc_val: 0.3500 time: 0.0186s\n",
      "Epoch: 0026 loss_train: 1.7285 acc_train: 0.2929 loss_val: 1.7267 acc_val: 0.3500 time: 0.0190s\n",
      "Epoch: 0027 loss_train: 1.7100 acc_train: 0.3000 loss_val: 1.7202 acc_val: 0.3500 time: 0.0186s\n",
      "Epoch: 0028 loss_train: 1.6918 acc_train: 0.3000 loss_val: 1.7134 acc_val: 0.3500 time: 0.0183s\n",
      "Epoch: 0029 loss_train: 1.7009 acc_train: 0.3000 loss_val: 1.7065 acc_val: 0.3467 time: 0.0188s\n",
      "Epoch: 0030 loss_train: 1.6952 acc_train: 0.3071 loss_val: 1.6994 acc_val: 0.3467 time: 0.0187s\n",
      "Epoch: 0031 loss_train: 1.6812 acc_train: 0.3143 loss_val: 1.6920 acc_val: 0.3467 time: 0.0195s\n",
      "Epoch: 0032 loss_train: 1.6401 acc_train: 0.3071 loss_val: 1.6843 acc_val: 0.3467 time: 0.0262s\n",
      "Epoch: 0033 loss_train: 1.6337 acc_train: 0.3143 loss_val: 1.6762 acc_val: 0.3467 time: 0.0218s\n",
      "Epoch: 0034 loss_train: 1.6378 acc_train: 0.3143 loss_val: 1.6676 acc_val: 0.3533 time: 0.0186s\n",
      "Epoch: 0035 loss_train: 1.6472 acc_train: 0.3000 loss_val: 1.6585 acc_val: 0.3567 time: 0.0191s\n",
      "Epoch: 0036 loss_train: 1.6187 acc_train: 0.3643 loss_val: 1.6487 acc_val: 0.3633 time: 0.0188s\n",
      "Epoch: 0037 loss_train: 1.5987 acc_train: 0.3429 loss_val: 1.6382 acc_val: 0.3667 time: 0.0189s\n",
      "Epoch: 0038 loss_train: 1.6009 acc_train: 0.3429 loss_val: 1.6273 acc_val: 0.3833 time: 0.0190s\n",
      "Epoch: 0039 loss_train: 1.5679 acc_train: 0.4143 loss_val: 1.6161 acc_val: 0.4033 time: 0.0227s\n",
      "Epoch: 0040 loss_train: 1.5303 acc_train: 0.4143 loss_val: 1.6045 acc_val: 0.4167 time: 0.0187s\n",
      "Epoch: 0041 loss_train: 1.5263 acc_train: 0.4286 loss_val: 1.5930 acc_val: 0.4333 time: 0.0258s\n",
      "Epoch: 0042 loss_train: 1.5126 acc_train: 0.4571 loss_val: 1.5814 acc_val: 0.4433 time: 0.0200s\n",
      "Epoch: 0043 loss_train: 1.5432 acc_train: 0.4429 loss_val: 1.5696 acc_val: 0.4567 time: 0.0209s\n",
      "Epoch: 0044 loss_train: 1.4797 acc_train: 0.5071 loss_val: 1.5575 acc_val: 0.4733 time: 0.0195s\n",
      "Epoch: 0045 loss_train: 1.4938 acc_train: 0.5071 loss_val: 1.5448 acc_val: 0.4800 time: 0.0183s\n",
      "Epoch: 0046 loss_train: 1.4107 acc_train: 0.5429 loss_val: 1.5314 acc_val: 0.5000 time: 0.0189s\n",
      "Epoch: 0047 loss_train: 1.3994 acc_train: 0.5643 loss_val: 1.5179 acc_val: 0.5167 time: 0.0182s\n",
      "Epoch: 0048 loss_train: 1.4143 acc_train: 0.5857 loss_val: 1.5039 acc_val: 0.5267 time: 0.0184s\n",
      "Epoch: 0049 loss_train: 1.3967 acc_train: 0.6214 loss_val: 1.4898 acc_val: 0.5267 time: 0.0185s\n",
      "Epoch: 0050 loss_train: 1.3692 acc_train: 0.6071 loss_val: 1.4750 acc_val: 0.5433 time: 0.0184s\n",
      "Epoch: 0051 loss_train: 1.3559 acc_train: 0.6286 loss_val: 1.4599 acc_val: 0.5533 time: 0.0183s\n",
      "Epoch: 0052 loss_train: 1.3447 acc_train: 0.6000 loss_val: 1.4447 acc_val: 0.5600 time: 0.0277s\n",
      "Epoch: 0053 loss_train: 1.3112 acc_train: 0.6643 loss_val: 1.4288 acc_val: 0.5767 time: 0.0199s\n",
      "Epoch: 0054 loss_train: 1.2908 acc_train: 0.6643 loss_val: 1.4130 acc_val: 0.5800 time: 0.0188s\n",
      "Epoch: 0055 loss_train: 1.2817 acc_train: 0.6357 loss_val: 1.3978 acc_val: 0.5967 time: 0.0194s\n",
      "Epoch: 0056 loss_train: 1.2662 acc_train: 0.6429 loss_val: 1.3830 acc_val: 0.6033 time: 0.0183s\n",
      "Epoch: 0057 loss_train: 1.2505 acc_train: 0.7000 loss_val: 1.3677 acc_val: 0.6100 time: 0.0182s\n",
      "Epoch: 0058 loss_train: 1.2193 acc_train: 0.6500 loss_val: 1.3529 acc_val: 0.6333 time: 0.0182s\n",
      "Epoch: 0059 loss_train: 1.2128 acc_train: 0.7143 loss_val: 1.3375 acc_val: 0.6533 time: 0.0183s\n",
      "Epoch: 0060 loss_train: 1.2028 acc_train: 0.7143 loss_val: 1.3217 acc_val: 0.6600 time: 0.0182s\n",
      "Epoch: 0061 loss_train: 1.1436 acc_train: 0.6857 loss_val: 1.3061 acc_val: 0.6700 time: 0.0210s\n",
      "Epoch: 0062 loss_train: 1.1300 acc_train: 0.7143 loss_val: 1.2905 acc_val: 0.6767 time: 0.0267s\n",
      "Epoch: 0063 loss_train: 1.1377 acc_train: 0.7286 loss_val: 1.2754 acc_val: 0.6933 time: 0.0190s\n",
      "Epoch: 0064 loss_train: 1.0859 acc_train: 0.7357 loss_val: 1.2606 acc_val: 0.6967 time: 0.0186s\n",
      "Epoch: 0065 loss_train: 1.0931 acc_train: 0.7500 loss_val: 1.2461 acc_val: 0.7033 time: 0.0181s\n",
      "Epoch: 0066 loss_train: 1.0901 acc_train: 0.7357 loss_val: 1.2319 acc_val: 0.7167 time: 0.0182s\n",
      "Epoch: 0067 loss_train: 1.0524 acc_train: 0.7429 loss_val: 1.2177 acc_val: 0.7200 time: 0.0185s\n",
      "Epoch: 0068 loss_train: 1.0199 acc_train: 0.7643 loss_val: 1.2032 acc_val: 0.7200 time: 0.0187s\n",
      "Epoch: 0069 loss_train: 1.0220 acc_train: 0.7143 loss_val: 1.1892 acc_val: 0.7233 time: 0.0187s\n",
      "Epoch: 0070 loss_train: 0.9936 acc_train: 0.7857 loss_val: 1.1752 acc_val: 0.7233 time: 0.0185s\n",
      "Epoch: 0071 loss_train: 1.0023 acc_train: 0.7857 loss_val: 1.1617 acc_val: 0.7267 time: 0.0205s\n",
      "Epoch: 0072 loss_train: 0.9315 acc_train: 0.7786 loss_val: 1.1488 acc_val: 0.7333 time: 0.0265s\n",
      "Epoch: 0073 loss_train: 0.9257 acc_train: 0.8143 loss_val: 1.1365 acc_val: 0.7300 time: 0.0200s\n",
      "Epoch: 0074 loss_train: 0.9219 acc_train: 0.7857 loss_val: 1.1251 acc_val: 0.7367 time: 0.0191s\n",
      "Epoch: 0075 loss_train: 0.8978 acc_train: 0.7786 loss_val: 1.1141 acc_val: 0.7400 time: 0.0185s\n",
      "Epoch: 0076 loss_train: 0.8826 acc_train: 0.7857 loss_val: 1.1026 acc_val: 0.7433 time: 0.0187s\n",
      "Epoch: 0077 loss_train: 0.9109 acc_train: 0.7929 loss_val: 1.0912 acc_val: 0.7467 time: 0.0186s\n",
      "Epoch: 0078 loss_train: 0.8734 acc_train: 0.7714 loss_val: 1.0798 acc_val: 0.7500 time: 0.0189s\n",
      "Epoch: 0079 loss_train: 0.8737 acc_train: 0.7714 loss_val: 1.0687 acc_val: 0.7500 time: 0.0192s\n",
      "Epoch: 0080 loss_train: 0.8633 acc_train: 0.7857 loss_val: 1.0585 acc_val: 0.7500 time: 0.0187s\n",
      "Epoch: 0081 loss_train: 0.8091 acc_train: 0.8286 loss_val: 1.0486 acc_val: 0.7600 time: 0.0213s\n",
      "Epoch: 0082 loss_train: 0.8551 acc_train: 0.8429 loss_val: 1.0374 acc_val: 0.7667 time: 0.0281s\n",
      "Epoch: 0083 loss_train: 0.8511 acc_train: 0.8500 loss_val: 1.0264 acc_val: 0.7700 time: 0.0192s\n",
      "Epoch: 0084 loss_train: 0.8248 acc_train: 0.7786 loss_val: 1.0164 acc_val: 0.7833 time: 0.0189s\n",
      "Epoch: 0085 loss_train: 0.7956 acc_train: 0.8143 loss_val: 1.0078 acc_val: 0.7867 time: 0.0183s\n",
      "Epoch: 0086 loss_train: 0.7795 acc_train: 0.8357 loss_val: 1.0001 acc_val: 0.7933 time: 0.0188s\n",
      "Epoch: 0087 loss_train: 0.7761 acc_train: 0.8143 loss_val: 0.9930 acc_val: 0.7900 time: 0.0189s\n",
      "Epoch: 0088 loss_train: 0.7934 acc_train: 0.7857 loss_val: 0.9877 acc_val: 0.7900 time: 0.0181s\n",
      "Epoch: 0089 loss_train: 0.7400 acc_train: 0.8643 loss_val: 0.9809 acc_val: 0.7867 time: 0.0188s\n",
      "Epoch: 0090 loss_train: 0.7753 acc_train: 0.8214 loss_val: 0.9735 acc_val: 0.7867 time: 0.0189s\n",
      "Epoch: 0091 loss_train: 0.7439 acc_train: 0.8214 loss_val: 0.9647 acc_val: 0.7867 time: 0.0189s\n",
      "Epoch: 0092 loss_train: 0.7632 acc_train: 0.8357 loss_val: 0.9559 acc_val: 0.7867 time: 0.0229s\n",
      "Epoch: 0093 loss_train: 0.7596 acc_train: 0.8214 loss_val: 0.9472 acc_val: 0.8000 time: 0.0217s\n",
      "Epoch: 0094 loss_train: 0.7312 acc_train: 0.8571 loss_val: 0.9388 acc_val: 0.8033 time: 0.0187s\n",
      "Epoch: 0095 loss_train: 0.7611 acc_train: 0.8214 loss_val: 0.9319 acc_val: 0.8067 time: 0.0183s\n",
      "Epoch: 0096 loss_train: 0.7022 acc_train: 0.8500 loss_val: 0.9250 acc_val: 0.8067 time: 0.0193s\n",
      "Epoch: 0097 loss_train: 0.6871 acc_train: 0.8286 loss_val: 0.9187 acc_val: 0.8033 time: 0.0187s\n",
      "Epoch: 0098 loss_train: 0.6875 acc_train: 0.8714 loss_val: 0.9130 acc_val: 0.8000 time: 0.0184s\n",
      "Epoch: 0099 loss_train: 0.6756 acc_train: 0.8643 loss_val: 0.9071 acc_val: 0.7967 time: 0.0188s\n",
      "Epoch: 0100 loss_train: 0.6494 acc_train: 0.8857 loss_val: 0.9009 acc_val: 0.8000 time: 0.0188s\n",
      "Epoch: 0101 loss_train: 0.6295 acc_train: 0.8857 loss_val: 0.8949 acc_val: 0.8000 time: 0.0189s\n",
      "Epoch: 0102 loss_train: 0.6389 acc_train: 0.8643 loss_val: 0.8891 acc_val: 0.8000 time: 0.0275s\n",
      "Epoch: 0103 loss_train: 0.6245 acc_train: 0.8857 loss_val: 0.8827 acc_val: 0.8000 time: 0.0200s\n",
      "Epoch: 0104 loss_train: 0.6134 acc_train: 0.8929 loss_val: 0.8759 acc_val: 0.8000 time: 0.0196s\n",
      "Epoch: 0105 loss_train: 0.6495 acc_train: 0.8714 loss_val: 0.8700 acc_val: 0.8033 time: 0.0191s\n",
      "Epoch: 0106 loss_train: 0.6015 acc_train: 0.8857 loss_val: 0.8645 acc_val: 0.8067 time: 0.0189s\n",
      "Epoch: 0107 loss_train: 0.5778 acc_train: 0.8929 loss_val: 0.8597 acc_val: 0.8067 time: 0.0192s\n",
      "Epoch: 0108 loss_train: 0.6725 acc_train: 0.8571 loss_val: 0.8557 acc_val: 0.8067 time: 0.0183s\n",
      "Epoch: 0109 loss_train: 0.6549 acc_train: 0.8500 loss_val: 0.8522 acc_val: 0.8067 time: 0.0189s\n",
      "Epoch: 0110 loss_train: 0.6098 acc_train: 0.8571 loss_val: 0.8484 acc_val: 0.8067 time: 0.0184s\n",
      "Epoch: 0111 loss_train: 0.5757 acc_train: 0.9000 loss_val: 0.8454 acc_val: 0.8067 time: 0.0198s\n",
      "Epoch: 0112 loss_train: 0.5928 acc_train: 0.8929 loss_val: 0.8426 acc_val: 0.8067 time: 0.0222s\n",
      "Epoch: 0113 loss_train: 0.5993 acc_train: 0.9071 loss_val: 0.8381 acc_val: 0.8067 time: 0.0257s\n",
      "Epoch: 0114 loss_train: 0.5878 acc_train: 0.8929 loss_val: 0.8338 acc_val: 0.8100 time: 0.0184s\n",
      "Epoch: 0115 loss_train: 0.6178 acc_train: 0.8786 loss_val: 0.8300 acc_val: 0.8100 time: 0.0187s\n",
      "Epoch: 0116 loss_train: 0.5529 acc_train: 0.9143 loss_val: 0.8261 acc_val: 0.8067 time: 0.0184s\n",
      "Epoch: 0117 loss_train: 0.5352 acc_train: 0.8786 loss_val: 0.8221 acc_val: 0.8067 time: 0.0187s\n",
      "Epoch: 0118 loss_train: 0.5495 acc_train: 0.8929 loss_val: 0.8175 acc_val: 0.8067 time: 0.0189s\n",
      "Epoch: 0119 loss_train: 0.5512 acc_train: 0.8786 loss_val: 0.8120 acc_val: 0.8133 time: 0.0191s\n",
      "Epoch: 0120 loss_train: 0.5446 acc_train: 0.8929 loss_val: 0.8078 acc_val: 0.8133 time: 0.0188s\n",
      "Epoch: 0121 loss_train: 0.5215 acc_train: 0.9214 loss_val: 0.8048 acc_val: 0.8133 time: 0.0220s\n",
      "Epoch: 0122 loss_train: 0.5638 acc_train: 0.9000 loss_val: 0.8016 acc_val: 0.8133 time: 0.0245s\n",
      "Epoch: 0123 loss_train: 0.5619 acc_train: 0.8643 loss_val: 0.7997 acc_val: 0.8133 time: 0.0213s\n",
      "Epoch: 0124 loss_train: 0.5471 acc_train: 0.8929 loss_val: 0.7985 acc_val: 0.8133 time: 0.0179s\n",
      "Epoch: 0125 loss_train: 0.4891 acc_train: 0.9214 loss_val: 0.7984 acc_val: 0.8133 time: 0.0176s\n",
      "Epoch: 0126 loss_train: 0.5102 acc_train: 0.9071 loss_val: 0.7975 acc_val: 0.8100 time: 0.0178s\n",
      "Epoch: 0127 loss_train: 0.4975 acc_train: 0.9000 loss_val: 0.7954 acc_val: 0.8133 time: 0.0182s\n",
      "Epoch: 0128 loss_train: 0.5214 acc_train: 0.9071 loss_val: 0.7918 acc_val: 0.8167 time: 0.0183s\n",
      "Epoch: 0129 loss_train: 0.5143 acc_train: 0.8929 loss_val: 0.7880 acc_val: 0.8200 time: 0.0186s\n",
      "Epoch: 0130 loss_train: 0.5191 acc_train: 0.9143 loss_val: 0.7844 acc_val: 0.8233 time: 0.0191s\n",
      "Epoch: 0131 loss_train: 0.4805 acc_train: 0.9429 loss_val: 0.7806 acc_val: 0.8233 time: 0.0197s\n",
      "Epoch: 0132 loss_train: 0.5262 acc_train: 0.8929 loss_val: 0.7776 acc_val: 0.8200 time: 0.0268s\n",
      "Epoch: 0133 loss_train: 0.4876 acc_train: 0.9214 loss_val: 0.7748 acc_val: 0.8200 time: 0.0202s\n",
      "Epoch: 0134 loss_train: 0.4894 acc_train: 0.9357 loss_val: 0.7721 acc_val: 0.8200 time: 0.0185s\n",
      "Epoch: 0135 loss_train: 0.4811 acc_train: 0.9143 loss_val: 0.7691 acc_val: 0.8167 time: 0.0184s\n",
      "Epoch: 0136 loss_train: 0.4796 acc_train: 0.9500 loss_val: 0.7672 acc_val: 0.8200 time: 0.0183s\n",
      "Epoch: 0137 loss_train: 0.4740 acc_train: 0.9143 loss_val: 0.7661 acc_val: 0.8200 time: 0.0183s\n",
      "Epoch: 0138 loss_train: 0.5198 acc_train: 0.9000 loss_val: 0.7661 acc_val: 0.8167 time: 0.0180s\n",
      "Epoch: 0139 loss_train: 0.4546 acc_train: 0.9286 loss_val: 0.7661 acc_val: 0.8200 time: 0.0189s\n",
      "Epoch: 0140 loss_train: 0.4772 acc_train: 0.9071 loss_val: 0.7650 acc_val: 0.8200 time: 0.0179s\n",
      "Epoch: 0141 loss_train: 0.4714 acc_train: 0.9286 loss_val: 0.7628 acc_val: 0.8167 time: 0.0196s\n",
      "Epoch: 0142 loss_train: 0.4840 acc_train: 0.9071 loss_val: 0.7605 acc_val: 0.8167 time: 0.0249s\n",
      "Epoch: 0143 loss_train: 0.4578 acc_train: 0.9214 loss_val: 0.7569 acc_val: 0.8167 time: 0.0215s\n",
      "Epoch: 0144 loss_train: 0.4590 acc_train: 0.9429 loss_val: 0.7531 acc_val: 0.8167 time: 0.0189s\n",
      "Epoch: 0145 loss_train: 0.4921 acc_train: 0.9214 loss_val: 0.7498 acc_val: 0.8233 time: 0.0187s\n",
      "Epoch: 0146 loss_train: 0.4565 acc_train: 0.9429 loss_val: 0.7468 acc_val: 0.8233 time: 0.0189s\n",
      "Epoch: 0147 loss_train: 0.4984 acc_train: 0.9286 loss_val: 0.7449 acc_val: 0.8200 time: 0.0185s\n",
      "Epoch: 0148 loss_train: 0.4546 acc_train: 0.9500 loss_val: 0.7431 acc_val: 0.8200 time: 0.0189s\n",
      "Epoch: 0149 loss_train: 0.4790 acc_train: 0.9214 loss_val: 0.7414 acc_val: 0.8167 time: 0.0189s\n",
      "Epoch: 0150 loss_train: 0.4726 acc_train: 0.9286 loss_val: 0.7396 acc_val: 0.8167 time: 0.0184s\n",
      "Epoch: 0151 loss_train: 0.4631 acc_train: 0.9357 loss_val: 0.7379 acc_val: 0.8167 time: 0.0209s\n",
      "Epoch: 0152 loss_train: 0.4426 acc_train: 0.9429 loss_val: 0.7367 acc_val: 0.8133 time: 0.0238s\n",
      "Epoch: 0153 loss_train: 0.4314 acc_train: 0.9357 loss_val: 0.7353 acc_val: 0.8167 time: 0.0214s\n",
      "Epoch: 0154 loss_train: 0.4287 acc_train: 0.9357 loss_val: 0.7338 acc_val: 0.8133 time: 0.0188s\n",
      "Epoch: 0155 loss_train: 0.4446 acc_train: 0.9143 loss_val: 0.7323 acc_val: 0.8133 time: 0.0181s\n",
      "Epoch: 0156 loss_train: 0.4349 acc_train: 0.9643 loss_val: 0.7289 acc_val: 0.8033 time: 0.0182s\n",
      "Epoch: 0157 loss_train: 0.4075 acc_train: 0.9357 loss_val: 0.7254 acc_val: 0.8067 time: 0.0180s\n",
      "Epoch: 0158 loss_train: 0.4388 acc_train: 0.9429 loss_val: 0.7227 acc_val: 0.8067 time: 0.0184s\n",
      "Epoch: 0159 loss_train: 0.4454 acc_train: 0.9429 loss_val: 0.7199 acc_val: 0.8067 time: 0.0180s\n",
      "Epoch: 0160 loss_train: 0.4293 acc_train: 0.9429 loss_val: 0.7169 acc_val: 0.8133 time: 0.0184s\n",
      "Epoch: 0161 loss_train: 0.4106 acc_train: 0.9500 loss_val: 0.7138 acc_val: 0.8133 time: 0.0189s\n",
      "Epoch: 0162 loss_train: 0.3753 acc_train: 0.9500 loss_val: 0.7111 acc_val: 0.8167 time: 0.0249s\n",
      "Epoch: 0163 loss_train: 0.4403 acc_train: 0.9071 loss_val: 0.7101 acc_val: 0.8167 time: 0.0207s\n",
      "Epoch: 0164 loss_train: 0.3897 acc_train: 0.9357 loss_val: 0.7104 acc_val: 0.8133 time: 0.0182s\n",
      "Epoch: 0165 loss_train: 0.4131 acc_train: 0.9500 loss_val: 0.7105 acc_val: 0.8200 time: 0.0185s\n",
      "Epoch: 0166 loss_train: 0.4365 acc_train: 0.9214 loss_val: 0.7104 acc_val: 0.8167 time: 0.0186s\n",
      "Epoch: 0167 loss_train: 0.4196 acc_train: 0.9357 loss_val: 0.7095 acc_val: 0.8133 time: 0.0189s\n",
      "Epoch: 0168 loss_train: 0.4082 acc_train: 0.9643 loss_val: 0.7069 acc_val: 0.8100 time: 0.0185s\n",
      "Epoch: 0169 loss_train: 0.3700 acc_train: 0.9429 loss_val: 0.7047 acc_val: 0.8033 time: 0.0189s\n",
      "Epoch: 0170 loss_train: 0.3947 acc_train: 0.9429 loss_val: 0.7031 acc_val: 0.8067 time: 0.0185s\n",
      "Epoch: 0171 loss_train: 0.3903 acc_train: 0.9357 loss_val: 0.7007 acc_val: 0.8167 time: 0.0200s\n",
      "Epoch: 0172 loss_train: 0.3664 acc_train: 0.9786 loss_val: 0.6978 acc_val: 0.8167 time: 0.0251s\n",
      "Epoch: 0173 loss_train: 0.3731 acc_train: 0.9500 loss_val: 0.6960 acc_val: 0.8167 time: 0.0213s\n",
      "Epoch: 0174 loss_train: 0.3746 acc_train: 0.9429 loss_val: 0.6942 acc_val: 0.8200 time: 0.0184s\n",
      "Epoch: 0175 loss_train: 0.3835 acc_train: 0.9429 loss_val: 0.6928 acc_val: 0.8167 time: 0.0189s\n",
      "Epoch: 0176 loss_train: 0.3603 acc_train: 0.9357 loss_val: 0.6922 acc_val: 0.8167 time: 0.0185s\n",
      "Epoch: 0177 loss_train: 0.3434 acc_train: 0.9643 loss_val: 0.6921 acc_val: 0.8133 time: 0.0184s\n",
      "Epoch: 0178 loss_train: 0.3994 acc_train: 0.9429 loss_val: 0.6928 acc_val: 0.8067 time: 0.0183s\n",
      "Epoch: 0179 loss_train: 0.4157 acc_train: 0.9286 loss_val: 0.6943 acc_val: 0.8067 time: 0.0186s\n",
      "Epoch: 0180 loss_train: 0.4058 acc_train: 0.9643 loss_val: 0.6938 acc_val: 0.8067 time: 0.0192s\n",
      "Epoch: 0181 loss_train: 0.3946 acc_train: 0.9500 loss_val: 0.6904 acc_val: 0.8033 time: 0.0195s\n",
      "Epoch: 0182 loss_train: 0.3787 acc_train: 0.9429 loss_val: 0.6886 acc_val: 0.8100 time: 0.0227s\n",
      "Epoch: 0183 loss_train: 0.4304 acc_train: 0.9286 loss_val: 0.6877 acc_val: 0.8133 time: 0.0215s\n",
      "Epoch: 0184 loss_train: 0.4223 acc_train: 0.9500 loss_val: 0.6869 acc_val: 0.8100 time: 0.0184s\n",
      "Epoch: 0185 loss_train: 0.3536 acc_train: 0.9571 loss_val: 0.6864 acc_val: 0.8100 time: 0.0177s\n",
      "Epoch: 0186 loss_train: 0.3972 acc_train: 0.9286 loss_val: 0.6870 acc_val: 0.8067 time: 0.0183s\n",
      "Epoch: 0187 loss_train: 0.3444 acc_train: 0.9500 loss_val: 0.6890 acc_val: 0.8033 time: 0.0189s\n",
      "Epoch: 0188 loss_train: 0.4058 acc_train: 0.9500 loss_val: 0.6911 acc_val: 0.8100 time: 0.0189s\n",
      "Epoch: 0189 loss_train: 0.3516 acc_train: 0.9714 loss_val: 0.6919 acc_val: 0.8067 time: 0.0183s\n",
      "Epoch: 0190 loss_train: 0.3476 acc_train: 0.9929 loss_val: 0.6912 acc_val: 0.8067 time: 0.0190s\n",
      "Epoch: 0191 loss_train: 0.3833 acc_train: 0.9429 loss_val: 0.6896 acc_val: 0.8067 time: 0.0191s\n",
      "Epoch: 0192 loss_train: 0.3838 acc_train: 0.9357 loss_val: 0.6874 acc_val: 0.8100 time: 0.0241s\n",
      "Epoch: 0193 loss_train: 0.3274 acc_train: 0.9429 loss_val: 0.6850 acc_val: 0.8100 time: 0.0214s\n",
      "Epoch: 0194 loss_train: 0.3574 acc_train: 0.9429 loss_val: 0.6825 acc_val: 0.8067 time: 0.0188s\n",
      "Epoch: 0195 loss_train: 0.3488 acc_train: 0.9429 loss_val: 0.6809 acc_val: 0.8067 time: 0.0185s\n",
      "Epoch: 0196 loss_train: 0.3294 acc_train: 0.9714 loss_val: 0.6804 acc_val: 0.8100 time: 0.0180s\n",
      "Epoch: 0197 loss_train: 0.3478 acc_train: 0.9571 loss_val: 0.6801 acc_val: 0.8000 time: 0.0186s\n",
      "Epoch: 0198 loss_train: 0.3511 acc_train: 0.9643 loss_val: 0.6783 acc_val: 0.8033 time: 0.0184s\n",
      "Epoch: 0199 loss_train: 0.3446 acc_train: 0.9500 loss_val: 0.6772 acc_val: 0.8033 time: 0.0183s\n",
      "Epoch: 0200 loss_train: 0.3350 acc_train: 0.9571 loss_val: 0.6772 acc_val: 0.8067 time: 0.0187s\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.6953 accuracy= 0.8350\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "Requirement already satisfied: scipy in /Users/shumpei/anaconda/lib/python3.6/site-packages (from torch_geometric) (1.1.0)\n",
      "Requirement already satisfied: h5py in /Users/shumpei/anaconda/lib/python3.6/site-packages (from torch_geometric) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/shumpei/anaconda/lib/python3.6/site-packages (from torch_geometric) (0.19.1)\n",
      "Requirement already satisfied: networkx in /Users/shumpei/anaconda/lib/python3.6/site-packages (from torch_geometric) (2.1)\n",
      "Collecting rdflib (from torch_geometric)\n",
      "  Using cached https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: pandas in /Users/shumpei/anaconda/lib/python3.6/site-packages (from torch_geometric) (0.24.2)\n",
      "Requirement already satisfied: numpy in /Users/shumpei/anaconda/lib/python3.6/site-packages (from torch_geometric) (1.15.2)\n",
      "Collecting plyfile (from torch_geometric)\n",
      "Requirement already satisfied: six in /Users/shumpei/anaconda/lib/python3.6/site-packages (from h5py->torch_geometric) (1.10.0)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /Users/shumpei/anaconda/lib/python3.6/site-packages (from networkx->torch_geometric) (4.3.0)\n",
      "Requirement already satisfied: pyparsing in /Users/shumpei/anaconda/lib/python3.6/site-packages (from rdflib->torch_geometric) (2.1.4)\n",
      "Collecting isodate (from rdflib->torch_geometric)\n",
      "  Using cached https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pytz>=2011k in /Users/shumpei/anaconda/lib/python3.6/site-packages (from pandas->torch_geometric) (2016.10)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/shumpei/anaconda/lib/python3.6/site-packages (from pandas->torch_geometric) (2.6.0)\n",
      "\u001b[31mconda 4.6.14 requires ruamel_yaml>=0.11.14, which is not installed.\u001b[0m\n",
      "\u001b[31mtensorflow 1.11.0 has requirement keras-applications>=1.0.5, but you'll have keras-applications 1.0.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow 1.11.0 has requirement keras-preprocessing>=1.0.3, but you'll have keras-preprocessing 1.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow 1.11.0 has requirement setuptools<=39.1.0, but you'll have setuptools 39.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow 1.11.0 has requirement tensorboard<1.12.0,>=1.11.0, but you'll have tensorboard 1.10.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: isodate, rdflib, plyfile, torch-geometric\n",
      "Successfully installed isodate-0.6.0 plyfile-0.7 rdflib-4.2.2 torch-geometric-1.3.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install torch_spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-af36dc1eddf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/shumpei/anaconda/lib/python3.6/site-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shumpei/anaconda/lib/python3.6/site-packages/torch_geometric/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetaLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shumpei/anaconda/lib/python3.6/site-packages/torch_geometric/nn/data_parallel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shumpei/anaconda/lib/python3.6/site-packages/torch_geometric/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0min_memory_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataListLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDenseDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shumpei/anaconda/lib/python3.6/site-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoalesce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m from torch_geometric.utils import (contains_isolated_nodes,\n\u001b[1;32m      9\u001b[0m                                    contains_self_loops, is_undirected)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_sparse'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
